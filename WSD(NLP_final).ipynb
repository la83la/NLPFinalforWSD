{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import nltk\n",
    "from lemminflect import getAllInflections, getAllLemmas\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import semcor\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "unmasker = pipeline('fill-mask', model='bert-base-uncased', top_k=10)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8293460607528687, 'purpose'),\n",
       " (0.06537218391895294, 'aim'),\n",
       " (0.026322726160287857, 'goal'),\n",
       " (0.013286370784044266, 'object'),\n",
       " (0.01147634256631136, 'function'),\n",
       " (0.009352392517030239, 'objective'),\n",
       " (0.00922191422432661, 'intention'),\n",
       " (0.0075449394062161446, 'intent'),\n",
       " (0.004589724820107222, 'task'),\n",
       " (0.004187298007309437, 'use')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test hugging face api\n",
    "sentense = \"Letters whose sole [MASK] is to make a political point will not be published.\"\n",
    "candidate = unmasker(sentense)\n",
    "result = []\n",
    "for i in range(len(candidate)):\n",
    "    result.append((candidate[i]['score'], candidate[i]['token_str']))\n",
    "    \n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAME_corpus len: 244506\n",
      "big_corpus len: 31564\n",
      "paper_corpus len: 123656\n",
      "party_test_corpus len: 70\n",
      "party_train_corpus len: 637\n"
     ]
    }
   ],
   "source": [
    "# load corpus \n",
    "with open('dataset/BAWE.txt', 'r', encoding='utf-8') as f:\n",
    "    BAME_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/big.txt', 'r', encoding='utf-8') as f:\n",
    "    big_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/paper.txt', 'r', encoding='utf-8') as f:\n",
    "    paper_corpus = f.read().strip().split('. ')\n",
    "with open('dataset/party_test.txt', 'r', encoding='utf-8') as f:\n",
    "    party_test_corpus = f.read().strip().split('\\n')\n",
    "with open('dataset/party_train.txt', 'r', encoding='utf-8') as f:\n",
    "    party_train_corpus = f.read().strip().split('\\n')\n",
    "    \n",
    "corpuses = [BAME_corpus, big_corpus, paper_corpus, party_test_corpus, party_train_corpus]\n",
    "cor_names = [\"BAME_corpus\", \"big_corpus\", \"paper_corpus\", \"party_test_corpus\", \"party_train_corpus\"]\n",
    "c_len = len(cor_names)\n",
    "for i in  range(c_len):\n",
    "    print(cor_names[i], \"len:\", len(corpuses[i]))\n",
    "    \n",
    "corpus_combine = BAME_corpus + big_corpus + paper_corpus + party_test_corpus + party_train_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load AKL words\n",
    "with open(\"data/noun.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    noun = f.read().strip().split(', ')\n",
    "with open(\"data/adj.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adj = f.read().strip().split(', ')\n",
    "with open(\"data/adv.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    adv = f.read().strip().split(', ')\n",
    "with open(\"data/verb.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    verb = f.read().strip().split(', ')\n",
    "with open(\"data/others.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    others = f.read().strip().split(', ')\n",
    "    \n",
    "AKL_words = [noun, adj, adv, verb, others]\n",
    "AKL_merge = noun + adj + adv + verb + others\n",
    "types = [\"noun\", \"adj\", \"adv\", \"verb\", \"others\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun words: 353\n",
      "adj words: 180\n",
      "adv words: 86\n",
      "verb words: 233\n",
      "others words: 75\n"
     ]
    }
   ],
   "source": [
    "a_len = len(AKL_words)\n",
    "for i in  range(a_len):\n",
    "    print(types[i], \"words:\", len(AKL_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the sentences\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    input: a string\n",
    "    output: a list\n",
    "    - transform to lower case\n",
    "    - remove the punctuation\n",
    "    - seperate the words by blank\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    punc = '!()-[]{};:\"\\,<\">./?@#$%^&*_~1234567890'\n",
    "    for p in punc: \n",
    "        text = text.replace(p, \"\")\n",
    "    return text\n",
    "\n",
    "corpus = []\n",
    "for cor in corpus_combine:\n",
    "    sentence = preprocess(cor)\n",
    "    corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_exist(st, base_word):\n",
    "    \"\"\"\"\n",
    "    若st 中有base_word的任何變形，回傳True\n",
    "    \"\"\"\n",
    "    tokens = st.split(' ')\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "        \n",
    "    for item in var_list:\n",
    "        if item in tokens:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def put_mask(sentense, base_word):\n",
    "    \"\"\"\n",
    "    把 [MASK] 放到第一個出現的 `base_word`各種變形\n",
    "    \"\"\"\n",
    "    tokens = sentense.split(' ')\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "            \n",
    "    rep_tokens = []\n",
    "    mask = 0 # Only put mask on the first appeared base word\n",
    "    for token in tokens:\n",
    "        add = 0\n",
    "        for item in var_list:\n",
    "            if token == item and mask== 0:\n",
    "                rep_tokens.append(\"[MASK]\")\n",
    "                add = 1\n",
    "                mask += 1\n",
    "        if add == 0:\n",
    "            rep_tokens.append(token)\n",
    "\n",
    "    res_sent = \" \".join(rep_tokens)\n",
    "    return res_sent, var_list\n",
    "\n",
    "def get_candidates(sentense, base_word):\n",
    "    \"\"\"\n",
    "    所有`base_word`的變形都不會納入candidates\n",
    "    \"\"\"\n",
    "    sentense, var_list = put_mask(sentense, base_word)\n",
    "    candidate = unmasker(sentense)\n",
    "    result = {}\n",
    "    for i in range(len(candidate)):\n",
    "        same = 0\n",
    "        for item in var_list:\n",
    "            if candidate[i]['token_str'] == item:\n",
    "                same = 1\n",
    "        if same == 0:\n",
    "            result[candidate[i]['token_str']] = candidate[i]['score']\n",
    "    return result\n",
    "\n",
    "# 檢查是否是AKL字\n",
    "def check_akl(word):\n",
    "    if word in AKL_merge:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "#得到相似度分數(use wup_similarity)\n",
    "def get_similarity_score(base_word, syn_word):\n",
    "    \"\"\"\n",
    "    return mean similarity score of this two words\n",
    "    compare all meaning\n",
    "    \"\"\"\n",
    "    base_sets = wn.synsets(base_word)\n",
    "    syn_sets = wn.synsets(syn_word)\n",
    "    n = len(base_sets)\n",
    "    m = len(syn_sets)\n",
    "    score = 0\n",
    "    for i in range(n):\n",
    "        for j in range(m):\n",
    "            try:\n",
    "                score += base_sets[i].wup_similarity(syn_sets[j])\n",
    "            except:\n",
    "                pass\n",
    "    try:\n",
    "        score = score/ (n*m)\n",
    "    except:\n",
    "        score = score / 1\n",
    "    return score\n",
    "\n",
    "# 把分太細的POS 縮小分類\n",
    "verb = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "adj = ['JJ', 'JJR', 'JJS']\n",
    "adv = ['RB', 'RBR', 'RBS']\n",
    "noun = ['NN', 'NNS', 'NNP', 'NNPS']\n",
    "\n",
    "# 拿到句子中的詞性\n",
    "def get_POS(sentense, target_word):\n",
    "    \"\"\"\n",
    "    回傳 `target_word` 在 `sentense`中的詞性\n",
    "    詞性種類: https://www.guru99.com/pos-tagging-chunking-nltk.html\n",
    "    \"\"\"\n",
    "    # print(\"sentense: \", sentense)\n",
    "    # print(\"target_word: \", target_word)\n",
    "    tokens = nltk.word_tokenize(sentense)\n",
    "    tag = nltk.pos_tag(tokens)\n",
    "    # all variation \n",
    "    vairation = getAllInflections(target_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "    flag = 0\n",
    "    mini_pos = \"\"\n",
    "    # print(var_list)\n",
    "    for tu in tag:\n",
    "        for var in var_list:\n",
    "            if tu[0] == var:\n",
    "                mini_pos = tu[1]\n",
    "                flag = 1\n",
    "                break\n",
    "        if flag == 1: # found\n",
    "            break\n",
    "    # print(\"mini_pos: \", mini_pos)\n",
    "    pos = []\n",
    "    if mini_pos in verb:\n",
    "        pos.append(\"verb\")\n",
    "    if mini_pos in adj:\n",
    "        pos.append(\"adj\")\n",
    "    if mini_pos in adv:\n",
    "        pos.append(\"adv\")\n",
    "    if mini_pos in noun:\n",
    "        pos.append(\"noun\")\n",
    "#     print(pos)\n",
    "#     print('-'*10)\n",
    "    return pos \n",
    "\n",
    "def same(cand_pos ,base_pos):\n",
    "    for i in cand_pos:\n",
    "        for j in base_pos:\n",
    "            if i==j:\n",
    "                return True\n",
    "    return False        \n",
    "    \n",
    "def calculate_weight_ver2(cand, sentense, base_word):\n",
    "    \"\"\"\n",
    "    input 1: the possible words dictionary\n",
    "    input 2: the sentense used\n",
    "    input 3: base word\n",
    "    \"\"\"\n",
    "    # print(cand)\n",
    "    data_items = cand.items()\n",
    "    data_list = list(data_items)\n",
    "    cand_df = pd.DataFrame(data_list, columns=['Words', 'Score'])\n",
    "    \n",
    "    # AKL part\n",
    "    c_len = len(cand_df)\n",
    "    for i in range(c_len):\n",
    "        if check_akl(cand_df['Words'][i]):\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.25\n",
    "#             print(\"in AKL\")\n",
    "            \n",
    "    # POS-tagging part\n",
    "    base_pos = get_POS(sentense, base_word) #取得base的詞性\n",
    "    vairation = getAllInflections(base_word)\n",
    "    var_list = set()\n",
    "    for types in vairation:\n",
    "        for item in vairation[types]:\n",
    "            var_list.add(item)\n",
    "    # print(\"base_pos\", base_pos)\n",
    "    for i in range(c_len): \n",
    "        sen_tokens = sentense.split()\n",
    "        for var in var_list:\n",
    "            if var in sen_tokens:\n",
    "                sent_temp = sentense.replace(var, cand_df['Words'][i])\n",
    "                break\n",
    "        cand_pos = get_POS(sent_temp, cand_df['Words'][i]) #取得candidate的詞性\n",
    "        # print(\"cand\", cand_df['Words'][i])\n",
    "        # print(\"cand_pos\", cand_pos)\n",
    "        # print(\"sent_temp: \", sent_temp)\n",
    "        if same(cand_pos ,base_pos):\n",
    "            cand_df['Score'][i] = cand_df['Score'][i] *1.5\n",
    "            # print(\"Same type\")\n",
    "#         else:\n",
    "#             print('Not Same type')\n",
    "    # Wordnet Similarity\n",
    "    for i in range(c_len):\n",
    "        cand_df['Score'][i] += get_similarity_score(base_word, cand_df['Words'][i])\n",
    "    \n",
    "    cand_df = cand_df.sort_values(by=['Score'], ascending=False).reset_index(drop=True)\n",
    "    return cand_df\n",
    "\n",
    "# 找兩個字最近的字義\n",
    "def find_sense_of_two_words(base_word, syn_word):\n",
    "    base_word = wn.synsets(base_word) #可增加詞性 base_word = wn.synsets(base_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "    syn_word = wn.synsets(syn_word) #可增加詞性 syn_word = wn.synsets(syn_word, pos=wn.VERB)  [VERB, NOUN, ADJ, ADV]\n",
    "#     print(\"-\"*10)\n",
    "#     print(\"find_sense_of_two_words\")\n",
    "#     print(\"base_word\", base_word)\n",
    "#     print(\"syn_word\", syn_word)\n",
    "#     print(\"-\"*10)\n",
    "    wup_similarity=[]\n",
    "    wup_similarity_dict={}\n",
    "    for i in base_word:\n",
    "        for j in syn_word:\n",
    "            if wn.wup_similarity(i, j) != None:\n",
    "                wup_similarity.append(wn.wup_similarity(i, j))\n",
    "                wup_similarity_dict[wn.wup_similarity(i, j)]=[i,j]\n",
    "    # print(\"wup_similarity\", wup_similarity)\n",
    "    # print(\"wup_similarity_dict\", wup_similarity_dict)  \n",
    "    \n",
    "    #找出相似度最大的值與sense    \n",
    "    similarity = max(wup_similarity)\n",
    "    #sense編號 \n",
    "    sense= wup_similarity_dict[max(wup_similarity)][0]\n",
    "    #字義\n",
    "    definition = wup_similarity_dict[max(wup_similarity)][0].definition()\n",
    "    \n",
    "#     sense1 = wup_similarity_dict[max(wup_similarity)][0].definition()\n",
    "#     sense2 = wup_similarity_dict[max(wup_similarity)][1].definition()\n",
    "#     print(\"sense1: \", wup_similarity_dict[max(wup_similarity)][0], sense1)\n",
    "#     print(\"sense2: \" ,  wup_similarity_dict[max(wup_similarity)][1], sense2)\n",
    "\n",
    "    return similarity, sense, definition  #propose和need相似度, propose和need相似度最接近的sense編號, 字義 \n",
    "\n",
    "\n",
    "def summary(sentence, base_word):\n",
    "    \"\"\"\n",
    "    輸入: sentence, target word\n",
    "    輸出: target word/ 例句/ 在此例句中找到最近的詞比對出來的字義 \n",
    "    \"\"\"\n",
    "    cand = get_candidates(sentence, base_word) # 找出可能的答案 \n",
    "    # print(\"candidate\", cand)\n",
    "    result_df = calculate_weight_ver2(cand, sentence, base_word) # 加權\n",
    "    r_len = len(result_df['Words'])\n",
    "    for i in range(r_len):\n",
    "        if(len(wn.synsets(result_df['Words'][i])))!= 0:\n",
    "            syn_final_word = result_df['Words'][i] # 拿第一名的結果\n",
    "            break\n",
    "    # print(\"base_word: \", base_word, \"syn_final_word\", syn_final_word)\n",
    "    similarity, sense, definition = find_sense_of_two_words(base_word, syn_final_word) # 找最近的字義\n",
    "    \n",
    "    similar = sense.lemma_names()\n",
    "    filter = []\n",
    "    for tmp in similar:\n",
    "        if tmp!=base_word:\n",
    "            filter.append(tmp)\n",
    "    result = '、'.join(filter)\n",
    "    \n",
    "    # 印出結果\n",
    "    print(f\"\"\"\n",
    "    Target Word：{base_word}\n",
    "\n",
    "    例句：{sentence}\n",
    "\n",
    "    --------------------\n",
    "\n",
    "    在此例句中 \"{base_word}\" 的字義是：{definition} \n",
    "    \n",
    "    以下列出同義字：{result}\n",
    "    \"\"\")\n",
    "    return sense\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_word = \"star\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of our base word sentense:  253\n"
     ]
    }
   ],
   "source": [
    "# get the sentense that contains base_word\n",
    "filter_corpus = []\n",
    "for cor in corpus: \n",
    "    if check_word_exist(cor, base_word): \n",
    "        filter_corpus.append(cor)\n",
    "print(\"length of our base word sentense: \", len(filter_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Target Word：star\n",
      "\n",
      "    例句：emails being popular and vulnerable are a star target for virus writers\n",
      "\n",
      "    --------------------\n",
      "\n",
      "    在此例句中 \"star\" 的字義是：be the star in a performance \n",
      "    \n",
      "    以下列出同義字：\n",
      "    \n",
      "sense: Synset('star.v.02')\n"
     ]
    }
   ],
   "source": [
    "sentense = filter_corpus[1]\n",
    "\n",
    "answer = summary(sentense, base_word)\n",
    "print(\"sense:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semcor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://man.hubwiz.com/docset/NLTK.docset/Contents/Resources/Documents/api/nltk.corpus.reader.html#module-nltk.corpus.reader.semcor\n",
    "\n",
    "https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/corpus/reader/semcor.html\n",
    "\n",
    "https://www.nltk.org/howto/corpus.html#chunked-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package semcor to\n",
      "[nltk_data]     C:\\Users\\WangHongWen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package semcor is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('semcor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(semcor.sents()[0]) # sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37176"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sents = len(semcor.sents()) # total number of sentences\n",
    "num_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:20<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# 一定會跑很久 -> 估計兩小時\n",
    "from tqdm import tqdm\n",
    "semcor_sence = []\n",
    "for i, j in zip(tqdm(range(100)),range(100)) :\n",
    "    semcor_sence.append(\" \".join(semcor.sents()[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sencor_sents = open(\"sencor_sentenses.txt\", \"w\")\n",
    "for element in semcor_sence:\n",
    "    sencor_sents.write(element + \"\\n\")\n",
    "sencor_sents.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read text file -> corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_semcor_idx(target_word):\n",
    "    filter_corpus = []\n",
    "    for cor in corpus: \n",
    "        sentense = preprocess(cor)\n",
    "        if target_word in sentense:\n",
    "            filter_corpus.append(sentense)\n",
    "    return filter_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Fulton County Grand Jury said Friday an investigation of Atlanta 's recent primary election produced `` no evidence '' that any irregularities took place .\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(semcor.sents()[0]) # sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(sentence_idx, target_word):\n",
    "    sent = semcor.tagged_sents(tag='sem')[0]\n",
    "    for word in sent:\n",
    "        if(type(word)!=list):\n",
    "            for lf in word.leaves():\n",
    "                if (lf == target_word):\n",
    "                    sense = word.label()\n",
    "                    return sense.synset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('probe.n.01')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_lemma(0, \"investigation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Target Word：investigation\n",
      "\n",
      "    例句：the fulton county grand jury said friday an investigation of atlanta 's recent primary election produced `` no evidence '' that any irregularities took place \n",
      "\n",
      "    --------------------\n",
      "\n",
      "    在此例句中 \"investigation\" 的字義是：the work of inquiring into something thoroughly and systematically \n",
      "    \n",
      "    以下列出同義字：investigating\n",
      "    \n",
      "sense: Synset('investigation.n.02')\n"
     ]
    }
   ],
   "source": [
    "sentense = \" \".join(semcor.sents()[0])\n",
    "sentense = preprocess(sentense)\n",
    "\n",
    "answer = summary(sentense, \"investigation\")\n",
    "print(\"sense:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "1. 我們判斷出的\n",
    "2. semcor 套件中已經有正確答案\n",
    "\n",
    "-> 1&2兩者去做比較！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates_words = [\"star\", \"mole\", \"galley\", \"cone\", \"bass\", \"bow\", \" taste\", \" interest\", \"issue\", \"duty\", \"sentence\", \"slug\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
